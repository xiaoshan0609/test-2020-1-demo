print('训练集size：',xgb_train.shape)

from sklearn.model_selection import KFold
import time
import psutil
from keras import backend as K
import lightgbm as lgb
from sklearn.metrics import f1_score

def f1_error(preds,dtrain):
    preds=preds.reshape(114684,3)
    print('输出',preds)
    print('输出规模',preds.shape)
    label=dtrain.get_label()
    y_pred=np.argmax(preds, axis=1)
    f1 = f1_score(y_pred,label, average='macro' )
    return 'f1-score', f1
params = {
        'boosting_type': 'gbdt',
        'objective': 'multiclass',
        'num_class':3,
        'learning_rate': 0.03,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': 2,
        'metric': 'multi_logloss',
        #'device':'gpu'
    }



kf=KFold(5,shuffle=True,random_state=520)
kfs=kf.split(xgb_train,xgb_train_label)
print('xgboost-5折校验')
ss=0
for i, (train_index,vaild_index) in enumerate(kfs):
    print('第{}次训练...'.format(i+1))
    train_x=xgb_train[train_index,:]
    train_y=xgb_train_label[train_index,:]
    train_y=train_y.reshape(train_y.shape[0])
    vaild_x=xgb_train[vaild_index,:]
    vaild_y=xgb_train_label[vaild_index,:]
    vaild_y=vaild_y.reshape(vaild_y.shape[0])
    
    train=lgb.Dataset(train_x,train_y)
    val=lgb.Dataset(vaild_x,vaild_y)
    
    val_t=lgb.Dataset(vaild_x)
    num_rounds=5000
    print("Memory free: {:2.4f} GB".format(psutil.virtual_memory().free / (1024**3)))
    lgb_model=lgb.train(params,train,num_rounds,valid_sets=[val],early_stopping_rounds=300)
    vaild_pre = lgb_model.predict(val_t,num_iteration=lgb_model.best_iteration)
    score=f1_error(vaild_pre,val)
    ss+=score[1]
    print('第{}次训练-smape:{:2.5f} \n'.format(i+1, score[1]))
    print("\n")
print('五折训练分数均值-smape:{:2.5f} \n'.format(ss/(i+1)))
lgb_model.save_model(r'/kaggle/working/test-2020-1-demo/model/js_lgb.model',num_iteration=-1)

print('lightgbm完成')