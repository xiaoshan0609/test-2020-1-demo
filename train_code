from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix   #混淆矩阵
import itertools
from keras.utils import to_categorical #数字标签转化为One-hot编码
from keras.models import Sequential
from keras.layers import Dense, Dropout,Flatten,Conv1D,MaxPool1D,BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
from keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from keras import optimizers
from keras.regularizers import l2
from random import shuffle
import os
import numpy as np
from tqdm import tqdm
from imblearn.under_sampling import RandomUnderSampler


import keras.backend.tensorflow_backend as ktf
# 指定GPUID, 第一块GPU可用
os.environ["CUDA_VISIBLE_DEVICES"] = "0，1，2，3"
# GPU 显存自动分配
config = tf.ConfigProto(allow_soft_placement=True)
config.gpu_options.allow_growth=True
# #config.gpu_options.per_process_gpu_memory_fraction = 0.3
session = tf.Session(config=config)
ktf.set_session(session)


filepath=r'./train'
file=os.listdir(filepath)
file.sort()
file.sort(key=lambda x:int(x[:-5]))

label_file=r'E:\planetdata\train_label.bin'
label = np.fromfile(label_file,dtype=np.int32)
label_list=label
label=to_categorical(label, num_classes = 3 )
print('总label个数:',len(label))

#下采样，解决样本不均衡问题
rus=RandomUnderSampler(random_state=0)
file_list=[i for i in range(len(file))]
file_list=np.array(file_list)
label_list=np.array(label_list)
file_list=file_list.reshape((file_list.shape[0],1))
label_list=label_list.reshape((label_list.shape[0]))
print('训练集不均衡===进行下采样')
X_resampled, y_resampled = rus.fit_resample(file_list, label_list)
X_resampled=X_resampled.reshape((X_resampled.shape[0],))
print('下采样完成===训练集均衡化')
print('类别1数目:',np.sum(y_resampled==0))
print('类别2数目:',np.sum(y_resampled==1))
print('类别3数目:',np.sum(y_resampled==2))

class MyCbk(ModelCheckpoint):
	def __init__(self, model, filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1):
		self.single_model = model
		super(MyCbk, self).__init__(filepath, monitor, verbose, save_best_only, save_weights_only, mode, period)
	def set_model(self, model):
		super(MyCbk, self).set_model(self.single_model)

#在训练集中分出一部分验证集
os.chdir(r'./train')
val_size=2000   #取出训练集中的十分之一作为验证集
#shuffle_list=[i for i in range(len(file))]
shuffle_list=X_resampled.tolist()  
shuffle(shuffle_list)
val_data = []
val_label = []
begin = val_size
end = begin + val_size
val_list = shuffle_list[begin:end]

print('======随机划分训练集与验证集=======')
print("验证集size：", val_size)

for index in tqdm(range(len(val_list))):
	vdata = np.fromfile(file[val_list[index]],dtype=np.float64)# 了
	vlabel = label[val_list[index]]
	val_data.append(vdata)
	val_label.append(vlabel)
val_data=np.array(val_data)
val_label=np.array(val_label)
val_data=val_data.reshape((val_data.shape[0],val_data.shape[1],1))
#val_label = val_label.reshape((val_label.shape[0],val_label.shape[1],1))

print('======训练集与验证集划分完成=======')

for index in range(len(val_list)):
    shuffle_list.remove(val_list[index])

def batch_generator(batch_size):
    global file
    global label
    global shuffle_list
    shuffle(shuffle_list)
    while 1:
        for i in range(len(shuffle_list)//batch_size):
            bat_data=[]
            bat_label=[]
            begin = i *batch_size
            end = begin + batch_size
            sub_list=shuffle_list[begin:end]
            for index in range(len(sub_list)):
                tdata=np.fromfile(file[sub_list[index]],dtype=np.float64)
                tlabel=label[sub_list[index]]
                bat_data.append(tdata)  
                bat_label.append(tlabel)
            bat_data=np.array(bat_data)
            bat_label=np.array(bat_label)
            bat_data=bat_data.reshape((bat_data.shape[0],bat_data.shape[1],1))
            #bat_label = bat_label.reshape((bat_label.shape[0],bat_label.shape[1],1))
            yield bat_data,bat_label


#模型搭建
model=Sequential()
model.add(Conv1D(filters=64,kernel_size=3,strides=1,activation='sigmoid',padding="same",input_shape=(2600,1)))
#对于一维卷积操作来说，这种input_shape是将前面一个维度定义为none
#写的时候就是(none,2600,1)，none指的是批次进来的个数
model.add(BatchNormalization())
model.add(Conv1D(filters=64,kernel_size=3,strides=1,activation='sigmoid',padding="same"))
model.add(MaxPool1D(pool_size=4))
model.add(Dropout(0.25))
model.add(Conv1D(filters=32,kernel_size=3,strides=1,activation='sigmoid',padding="same"))
model.add(BatchNormalization())
model.add(Conv1D(filters=32,kernel_size=3,strides=1,activation='relu',padding="same"))
model.add(BatchNormalization())
model.add(MaxPool1D(pool_size=4))
model.add(Conv1D(filters=32,kernel_size=3,strides=1,activation='relu',padding="same"))
model.add(BatchNormalization())
model.add(MaxPool1D(pool_size=2,strides=2))
model.add(Flatten())
model.add(Dense(128, activation='sigmoid'))
model.add(Dense(32, activation='sigmoid'))     
model.add(Dropout(0.25))
# 输出层
model.add(Dense(3, activation='softmax'))
opt=optimizers.Adam(lr=0.001, epsilon=1e-8, decay=1e-4)
model.compile(optimizer = opt , loss = "categorical_crossentropy", metrics=["accuracy"])
model.summary()

learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc', 
                                            patience=3,
                                            verbose=1,
                                            factor=0.5,
                                            min_lr=0.000001
                                            )          
                                            
ckpt = "model_{epoch:02d}-{val_acc:.2f}.hdf5"
checkpoint = ModelCheckpoint(os.path.join(r'E:\planetdata\checkpoints', ckpt), monitor='val_acc', save_weights_only=False, verbose=1, save_best_only=True)
earlystopping = EarlyStopping(monitor='val_acc', verbose=1, patience=3, restore_best_weights=True)

epoch=15
batch_size=800
batch_num=len(shuffle_list)//batch_size
print('开始训练')
print('epoch:',epoch)
print('batchsize:',batch_size)
print('batch_num:',batch_num)




model.fit_generator(
                    batch_generator(batch_size),steps_per_epoch=batch_num,
                    epochs=epoch,
                    callbacks=[learning_rate_reduction,checkpoint,earlystopping],
                    workers=1,
                    use_multiprocessing=False,validation_data=(val_data,val_label)
                    )


model.save_weights(r'E:\planetdata\model\planetdata1.hdf5')
